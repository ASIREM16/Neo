\hypertarget{classnn_1_1nn}{}\section{nn.\+nn Class Reference}
\label{classnn_1_1nn}\index{nn.\+nn@{nn.\+nn}}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, layer\+\_\+dimensions, \hyperlink{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}{activations})
\item 
def \hyperlink{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}{initialize\+\_\+parameters} (self, layer\+\_\+dimensions)
\item 
def \hyperlink{classnn_1_1nn_a15ee3f3e18ebae83c904b3441ece897d}{check\+\_\+activations} (self)
\item 
def \hyperlink{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}{forward} (self, net\+\_\+input)
\item 
def \hyperlink{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}{forward\+\_\+upto} (self, net\+\_\+input, layer\+\_\+num)
\item 
def \hyperlink{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}{M\+S\+E\+Loss} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}{Cross\+Entropy\+Loss} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}{output\+\_\+backward} (self, prediction, mapping)
\item 
def \hyperlink{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}{deactivate} (self, dA, n\+\_\+layer)
\item 
def \hyperlink{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}{linear\+\_\+backward} (self, dA, n\+\_\+layer)
\item 
def \hyperlink{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}{backward} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_a1c967a0ae06ef2a7e8077cc9aa29aabc}{\+\_\+\+\_\+str\+\_\+\+\_\+} (self)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}{parameters}
\item 
\hyperlink{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}{cache}
\item 
\hyperlink{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}{activations}
\item 
\hyperlink{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}{cost\+\_\+function}
\item 
\hyperlink{classnn_1_1nn_a11943885141afc3a47049b9d2769fd1b}{lamb}
\item 
\hyperlink{classnn_1_1nn_a50e9804c2895867c31833f877c7d5d60}{grads}
\end{DoxyCompactItemize}
\subsection*{Private Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_a1cefed6bf4fd61d437e68c901a15fda2}{\+\_\+\+\_\+activate} (self, Z, n\+\_\+layer=1)
\end{DoxyCompactItemize}
\subsection*{Static Private Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_a78173fb77f846a71e27f097b29e701f8}{\+\_\+\+\_\+linear\+\_\+forward} (A\+\_\+prev, W, b)
\end{DoxyCompactItemize}


\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}\label{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions,  }\item[{}]{activations }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initializes networks's weights and other useful variables.

:param layer_dimensions:
:param activations: To store the activation for each layer
-Parameters contains weights of the layer in form {'Wi':[],'bi':[]}
-Cache contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i
 is layer number.
-activations contains the names of activation function used for that layer
-cost_function  contains the name of cost function to be used
-lamb contains the regularization hyper-parameter
-grads contains the gradients calculated during back-prop in form {'dA(i-1)':[],'dWi':[],'dbi':[]}
\end{DoxyVerb}
 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classnn_1_1nn_a1cefed6bf4fd61d437e68c901a15fda2}\label{classnn_1_1nn_a1cefed6bf4fd61d437e68c901a15fda2}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+activate@{\+\_\+\+\_\+activate}}
\index{\+\_\+\+\_\+activate@{\+\_\+\+\_\+activate}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+activate()}{\_\_activate()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+activate (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{Z,  }\item[{}]{n\+\_\+layer = {\ttfamily 1} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [private]}}

\begin{DoxyVerb}Activate the given layer(Z) using the activation function specified by
'type'.

Note: This function treats 1 as starting index!
      First layer's index is 1.

:param Z: Layer to activate
:param n_layer: Layer's index
:return: Activated layer and activation cache
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a78173fb77f846a71e27f097b29e701f8}\label{classnn_1_1nn_a78173fb77f846a71e27f097b29e701f8}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+linear\+\_\+forward@{\+\_\+\+\_\+linear\+\_\+forward}}
\index{\+\_\+\+\_\+linear\+\_\+forward@{\+\_\+\+\_\+linear\+\_\+forward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+linear\+\_\+forward()}{\_\_linear\_forward()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+linear\+\_\+forward (\begin{DoxyParamCaption}\item[{}]{A\+\_\+prev,  }\item[{}]{W,  }\item[{}]{b }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}, {\ttfamily [private]}}

\begin{DoxyVerb}Linear forward to the current layer using previous activations.

:param A_prev: Previous Layer's activation
:param W: Weights for current layer
:param b: Biases for current layer
:return: Linear cache and current calculated layer
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a1c967a0ae06ef2a7e8077cc9aa29aabc}\label{classnn_1_1nn_a1c967a0ae06ef2a7e8077cc9aa29aabc}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+str\+\_\+\+\_\+@{\+\_\+\+\_\+str\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+str\+\_\+\+\_\+@{\+\_\+\+\_\+str\+\_\+\+\_\+}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+str\+\_\+\+\_\+()}{\_\_str\_\_()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+str\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}:Return: the network architecture and connectivity
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}\label{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}} 
\index{nn\+::nn@{nn\+::nn}!backward@{backward}}
\index{backward@{backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{backward()}{backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Backward propagates through the network and stores useful calculations

:param prediction: Output of neural net
:param mapping: Correct output of the function
:return : None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a15ee3f3e18ebae83c904b3441ece897d}\label{classnn_1_1nn_a15ee3f3e18ebae83c904b3441ece897d}} 
\index{nn\+::nn@{nn\+::nn}!check\+\_\+activations@{check\+\_\+activations}}
\index{check\+\_\+activations@{check\+\_\+activations}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{check\+\_\+activations()}{check\_activations()}}
{\footnotesize\ttfamily def nn.\+nn.\+check\+\_\+activations (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Checks if activations for all layers are present. Adds 'None' if no activations are provided for a particular layer.

:returns: None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}\label{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}} 
\index{nn\+::nn@{nn\+::nn}!Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}}
\index{Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{Cross\+Entropy\+Loss()}{CrossEntropyLoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+Cross\+Entropy\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the cross entropy loss between output of the network and the real mappings of a function
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}\label{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}} 
\index{nn\+::nn@{nn\+::nn}!deactivate@{deactivate}}
\index{deactivate@{deactivate}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{deactivate()}{deactivate()}}
{\footnotesize\ttfamily def nn.\+nn.\+deactivate (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{dA,  }\item[{}]{n\+\_\+layer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the derivate of dA by deactivating the layer

:param dA: Activated derivative of the layer
:n_layer: Layer number to be deactivated
:return: deact=> derivative of activation 
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}\label{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}} 
\index{nn\+::nn@{nn\+::nn}!forward@{forward}}
\index{forward@{forward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input }\end{DoxyParamCaption})}

\begin{DoxyVerb}To forward propagate the entire Network.

:param net_input: Contains the input to the Network
:return: Output of the network
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}\label{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}} 
\index{nn\+::nn@{nn\+::nn}!forward\+\_\+upto@{forward\+\_\+upto}}
\index{forward\+\_\+upto@{forward\+\_\+upto}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward\+\_\+upto()}{forward\_upto()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward\+\_\+upto (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input,  }\item[{}]{layer\+\_\+num }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates forward prop upto layer_num.

:param net_input: Contains the input to the Network
:param layer_num: Layer up to which forward prop is to be calculated
:return: Activations of layer layer_num
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}\label{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}} 
\index{nn\+::nn@{nn\+::nn}!initialize\+\_\+parameters@{initialize\+\_\+parameters}}
\index{initialize\+\_\+parameters@{initialize\+\_\+parameters}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{initialize\+\_\+parameters()}{initialize\_parameters()}}
{\footnotesize\ttfamily def nn.\+nn.\+initialize\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions }\end{DoxyParamCaption})}

\begin{DoxyVerb}Xavier initialization of weights of a network described by given layer
dimensions.

:param layer_dimensions: Dimensions to layers of the network
:return: None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}\label{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}} 
\index{nn\+::nn@{nn\+::nn}!linear\+\_\+backward@{linear\+\_\+backward}}
\index{linear\+\_\+backward@{linear\+\_\+backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{linear\+\_\+backward()}{linear\_backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+linear\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{dA,  }\item[{}]{n\+\_\+layer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates linear backward propragation for layer denoted by n_layer

:param dA: Derivative of cost w.r.t this layer
:param n_layer: layer number
:return : dZ,dW,db,dA_prev
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}\label{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}} 
\index{nn\+::nn@{nn\+::nn}!M\+S\+E\+Loss@{M\+S\+E\+Loss}}
\index{M\+S\+E\+Loss@{M\+S\+E\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{M\+S\+E\+Loss()}{MSELoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+M\+S\+E\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the Mean Squared error with regularization cost(if provided) between output of the network and the real
mappings of a function.
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}\label{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}} 
\index{nn\+::nn@{nn\+::nn}!output\+\_\+backward@{output\+\_\+backward}}
\index{output\+\_\+backward@{output\+\_\+backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{output\+\_\+backward()}{output\_backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+output\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mapping }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the derivative of the output layer(dA)

:param prediction: Output of neural net
:param mapping: Correct output of the function
:param cost_type: Type of Cost function used
:return: Derivative of output layer, dA  
\end{DoxyVerb}
 

\subsection{Member Data Documentation}
\mbox{\Hypertarget{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}\label{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}} 
\index{nn\+::nn@{nn\+::nn}!activations@{activations}}
\index{activations@{activations}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{activations}{activations}}
{\footnotesize\ttfamily nn.\+nn.\+activations}

\mbox{\Hypertarget{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}\label{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}} 
\index{nn\+::nn@{nn\+::nn}!cache@{cache}}
\index{cache@{cache}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{cache}{cache}}
{\footnotesize\ttfamily nn.\+nn.\+cache}

\mbox{\Hypertarget{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}\label{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}} 
\index{nn\+::nn@{nn\+::nn}!cost\+\_\+function@{cost\+\_\+function}}
\index{cost\+\_\+function@{cost\+\_\+function}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{cost\+\_\+function}{cost\_function}}
{\footnotesize\ttfamily nn.\+nn.\+cost\+\_\+function}

\mbox{\Hypertarget{classnn_1_1nn_a50e9804c2895867c31833f877c7d5d60}\label{classnn_1_1nn_a50e9804c2895867c31833f877c7d5d60}} 
\index{nn\+::nn@{nn\+::nn}!grads@{grads}}
\index{grads@{grads}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{grads}{grads}}
{\footnotesize\ttfamily nn.\+nn.\+grads}

\mbox{\Hypertarget{classnn_1_1nn_a11943885141afc3a47049b9d2769fd1b}\label{classnn_1_1nn_a11943885141afc3a47049b9d2769fd1b}} 
\index{nn\+::nn@{nn\+::nn}!lamb@{lamb}}
\index{lamb@{lamb}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{lamb}{lamb}}
{\footnotesize\ttfamily nn.\+nn.\+lamb}

\mbox{\Hypertarget{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}\label{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}} 
\index{nn\+::nn@{nn\+::nn}!parameters@{parameters}}
\index{parameters@{parameters}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{parameters}{parameters}}
{\footnotesize\ttfamily nn.\+nn.\+parameters}



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/\hyperlink{nn_8py}{nn.\+py}\end{DoxyCompactItemize}
