\hypertarget{classnn_1_1nn}{}\section{nn.\+nn Class Reference}
\label{classnn_1_1nn}\index{nn.\+nn@{nn.\+nn}}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, layer\+\_\+dimensions, activations)
\item 
def \hyperlink{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}{initialize\+\_\+parameters} (self, layer\+\_\+dimensions)
\item 
def \hyperlink{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}{forward} (self, net\+\_\+input)
\item 
def \hyperlink{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}{forward\+\_\+upto} (self, net\+\_\+input, layer\+\_\+num)
\item 
def \hyperlink{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}{M\+S\+E\+Loss} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}{Cross\+Entropy\+Loss} (self, prediction, mappings)
\end{DoxyCompactItemize}
\subsection*{Static Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_a1ca8bcabd14da37dc25bbf49242dfd2c}{output\+\_\+backward} (prediction, mapping, cost\+\_\+type)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}\label{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}} 
{\bfseries parameters}
\item 
\mbox{\Hypertarget{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}\label{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}} 
{\bfseries cache}
\item 
\mbox{\Hypertarget{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}\label{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}} 
{\bfseries activations}
\item 
\mbox{\Hypertarget{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}\label{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}} 
{\bfseries cost\+\_\+function}
\end{DoxyCompactItemize}


\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}\label{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions,  }\item[{}]{activations }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initializes networks's weights and other useful variables.

Parameters contains weights of the layer in form {'Wi':[],'bi':[]}
Cache contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i
is layer number.

:param layer_dimensions:
:param activations: To store the activation for each layer
\end{DoxyVerb}
 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}\label{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}} 
\index{nn\+::nn@{nn\+::nn}!Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}}
\index{Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{Cross\+Entropy\+Loss()}{CrossEntropyLoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+Cross\+Entropy\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the cross entropy loss between output of the network and the real mappings of a function
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}\label{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}} 
\index{nn\+::nn@{nn\+::nn}!forward@{forward}}
\index{forward@{forward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input }\end{DoxyParamCaption})}

\begin{DoxyVerb}To forward propagate the entire Network.

:param net_input: Contains the input to the Network
:return: Output of the network
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}\label{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}} 
\index{nn\+::nn@{nn\+::nn}!forward\+\_\+upto@{forward\+\_\+upto}}
\index{forward\+\_\+upto@{forward\+\_\+upto}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward\+\_\+upto()}{forward\_upto()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward\+\_\+upto (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input,  }\item[{}]{layer\+\_\+num }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates forward prop upto layer_num.

:param net_input: Contains the input to the Network
:param layer_num: Layer up to which forward prop is to be calculated
:return: Activations of layer layer_num
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}\label{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}} 
\index{nn\+::nn@{nn\+::nn}!initialize\+\_\+parameters@{initialize\+\_\+parameters}}
\index{initialize\+\_\+parameters@{initialize\+\_\+parameters}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{initialize\+\_\+parameters()}{initialize\_parameters()}}
{\footnotesize\ttfamily def nn.\+nn.\+initialize\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions }\end{DoxyParamCaption})}

\begin{DoxyVerb}Random initialization of weights of a network described by given layer
dimensions.

:param layer_dimensions: Dimensions to layers of the network
:return: None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}\label{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}} 
\index{nn\+::nn@{nn\+::nn}!M\+S\+E\+Loss@{M\+S\+E\+Loss}}
\index{M\+S\+E\+Loss@{M\+S\+E\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{M\+S\+E\+Loss()}{MSELoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+M\+S\+E\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the Mean Squared error between output of the network and the real mappings of a function.
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a1ca8bcabd14da37dc25bbf49242dfd2c}\label{classnn_1_1nn_a1ca8bcabd14da37dc25bbf49242dfd2c}} 
\index{nn\+::nn@{nn\+::nn}!output\+\_\+backward@{output\+\_\+backward}}
\index{output\+\_\+backward@{output\+\_\+backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{output\+\_\+backward()}{output\_backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+output\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{prediction,  }\item[{}]{mapping,  }\item[{}]{cost\+\_\+type }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

\begin{DoxyVerb}Calculates the derivative of the output layer(dA)

:param prediction: Output of neural net
:param cost_type: Type of Cost function used
:return: Derivative of output layer, dA  
\end{DoxyVerb}
 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/nn.\+py\end{DoxyCompactItemize}
