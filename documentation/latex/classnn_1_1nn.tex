\hypertarget{classnn_1_1nn}{}\section{nn.\+nn Class Reference}
\label{classnn_1_1nn}\index{nn.\+nn@{nn.\+nn}}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, layer\+\_\+dimensions, activations)
\item 
def \hyperlink{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}{initialize\+\_\+parameters} (self, layer\+\_\+dimensions)
\item 
def \hyperlink{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}{forward} (self, net\+\_\+input)
\item 
def \hyperlink{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}{forward\+\_\+upto} (self, net\+\_\+input, layer\+\_\+num)
\item 
def \hyperlink{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}{M\+S\+E\+Loss} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}{Cross\+Entropy\+Loss} (self, prediction, mappings)
\item 
def \hyperlink{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}{output\+\_\+backward} (self, prediction, mapping)
\item 
def \hyperlink{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}{deactivate} (self, dA, n\+\_\+layer)
\item 
def \hyperlink{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}{linear\+\_\+backward} (self, dA, n\+\_\+layer)
\item 
def \hyperlink{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}{backward} (self, prediction, mappings)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}\label{classnn_1_1nn_a69da89bd6d17dbc8596ab586b7678237}} 
{\bfseries parameters}
\item 
\mbox{\Hypertarget{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}\label{classnn_1_1nn_a1f75a6242fc9ee82ca2632e18979b4d3}} 
{\bfseries cache}
\item 
\mbox{\Hypertarget{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}\label{classnn_1_1nn_acb7fb4cc0db120b007ef1ab1f82d7ba0}} 
{\bfseries activations}
\item 
\mbox{\Hypertarget{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}\label{classnn_1_1nn_adb4a96a154d03db3722022600e134c7f}} 
{\bfseries cost\+\_\+function}
\item 
\mbox{\Hypertarget{classnn_1_1nn_a50e9804c2895867c31833f877c7d5d60}\label{classnn_1_1nn_a50e9804c2895867c31833f877c7d5d60}} 
{\bfseries grads}
\end{DoxyCompactItemize}


\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}\label{classnn_1_1nn_ad7304c7932970a07bf2869e97b79e0b5}} 
\index{nn\+::nn@{nn\+::nn}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def nn.\+nn.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions,  }\item[{}]{activations }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initializes networks's weights and other useful variables.

:param layer_dimensions:
:param activations: To store the activation for each layer
-Parameters contains weights of the layer in form {'Wi':[],'bi':[]}
-Cache contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i
 is layer number.
-activations contains the names of activation function used for that layer
-grads contains the gradients calculated during back-prop in form {'dA(i-1)':[],'dWi':[],'dbi':[]}
\end{DoxyVerb}
 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}\label{classnn_1_1nn_a53a7beb698fe127ebb1f636fccbaa126}} 
\index{nn\+::nn@{nn\+::nn}!backward@{backward}}
\index{backward@{backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{backward()}{backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Backward propagates through the network and stores useful calculations

:param prediction: Output of neural net
:param mapping: Correct output of the function
:return : None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}\label{classnn_1_1nn_a822299322a0b513f8985e61096be45bc}} 
\index{nn\+::nn@{nn\+::nn}!Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}}
\index{Cross\+Entropy\+Loss@{Cross\+Entropy\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{Cross\+Entropy\+Loss()}{CrossEntropyLoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+Cross\+Entropy\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the cross entropy loss between output of the network and the real mappings of a function
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}\label{classnn_1_1nn_a475811849fd370a47eb0e3b7bc09b283}} 
\index{nn\+::nn@{nn\+::nn}!deactivate@{deactivate}}
\index{deactivate@{deactivate}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{deactivate()}{deactivate()}}
{\footnotesize\ttfamily def nn.\+nn.\+deactivate (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{dA,  }\item[{}]{n\+\_\+layer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the derivate of dA by deactivating the layer

:param dA: Activated derivative of the layer
:n_layer: Layer number to be deactivated
:return: deact=> derivative of activation 
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}\label{classnn_1_1nn_ae07002745b03901814d92ac66fe87781}} 
\index{nn\+::nn@{nn\+::nn}!forward@{forward}}
\index{forward@{forward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input }\end{DoxyParamCaption})}

\begin{DoxyVerb}To forward propagate the entire Network.

:param net_input: Contains the input to the Network
:return: Output of the network
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}\label{classnn_1_1nn_ac3d2b61ed992dc615eacda8e75a61a2b}} 
\index{nn\+::nn@{nn\+::nn}!forward\+\_\+upto@{forward\+\_\+upto}}
\index{forward\+\_\+upto@{forward\+\_\+upto}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{forward\+\_\+upto()}{forward\_upto()}}
{\footnotesize\ttfamily def nn.\+nn.\+forward\+\_\+upto (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{net\+\_\+input,  }\item[{}]{layer\+\_\+num }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates forward prop upto layer_num.

:param net_input: Contains the input to the Network
:param layer_num: Layer up to which forward prop is to be calculated
:return: Activations of layer layer_num
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}\label{classnn_1_1nn_a9821fed1369b4d709fe297fe9e07d97b}} 
\index{nn\+::nn@{nn\+::nn}!initialize\+\_\+parameters@{initialize\+\_\+parameters}}
\index{initialize\+\_\+parameters@{initialize\+\_\+parameters}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{initialize\+\_\+parameters()}{initialize\_parameters()}}
{\footnotesize\ttfamily def nn.\+nn.\+initialize\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{layer\+\_\+dimensions }\end{DoxyParamCaption})}

\begin{DoxyVerb}Random initialization of weights of a network described by given layer
dimensions.

:param layer_dimensions: Dimensions to layers of the network
:return: None
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}\label{classnn_1_1nn_a0863e90359fa30486c7161cd31c5b4e7}} 
\index{nn\+::nn@{nn\+::nn}!linear\+\_\+backward@{linear\+\_\+backward}}
\index{linear\+\_\+backward@{linear\+\_\+backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{linear\+\_\+backward()}{linear\_backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+linear\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{dA,  }\item[{}]{n\+\_\+layer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates linear backward propragation for layer denoted by n_layer

:param dA: Derivative of cost w.r.t this layer
:param n_layer: layer number
:return : dZ,dW,db,dA_prev
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}\label{classnn_1_1nn_ae74a0f21e8722ea82a0f94135a81a348}} 
\index{nn\+::nn@{nn\+::nn}!M\+S\+E\+Loss@{M\+S\+E\+Loss}}
\index{M\+S\+E\+Loss@{M\+S\+E\+Loss}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{M\+S\+E\+Loss()}{MSELoss()}}
{\footnotesize\ttfamily def nn.\+nn.\+M\+S\+E\+Loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mappings }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the Mean Squared error between output of the network and the real mappings of a function.
Changes cost_function to appropriate value

:param prediction: Output of the neural net
:param mappings: Real outputs of a function
:return: Mean squared error b/w output and mappings
\end{DoxyVerb}
 \mbox{\Hypertarget{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}\label{classnn_1_1nn_afa7d1462872fae95fe71f27ed00bf7ae}} 
\index{nn\+::nn@{nn\+::nn}!output\+\_\+backward@{output\+\_\+backward}}
\index{output\+\_\+backward@{output\+\_\+backward}!nn\+::nn@{nn\+::nn}}
\subsubsection{\texorpdfstring{output\+\_\+backward()}{output\_backward()}}
{\footnotesize\ttfamily def nn.\+nn.\+output\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction,  }\item[{}]{mapping }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the derivative of the output layer(dA)

:param prediction: Output of neural net
:param mapping: Correct output of the function
:param cost_type: Type of Cost function used
:return: Derivative of output layer, dA  
\end{DoxyVerb}
 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/nn.\+py\end{DoxyCompactItemize}
